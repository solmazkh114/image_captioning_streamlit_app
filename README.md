The aim of this project is to show and compare the ability of encoder-decoder models (VGG & LSTM) and a LLM model (Blip) which is based on transformer-architecture. Considering the resource
limit for training the VGG &LSTM  model, the performance of this model is not as good as the transformer model, although it is promising. To make it easy to compare these two, we provided a
streamlit- demo app that can be used after cloning the repository by this terminal command:

``` streamlit run main.py```
